{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from heapq import *\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huffman Compression Algorithm: Survey and Implementation from Scratch\n",
    "## Author: Kostadin Kostadinov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "Explaining what is a compression, what are the differences between lossless and lossy compression, what is entropy encoding, what are the Huffman trees, what is a Pseudo-EOF, implementation of the Huffman Compression Algorithm for text compressing from scratch and comparing him to other lossless compression algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding and/or using such a code proceeds by means of Huffman coding - an algorithm developed by David A. Huffman.\n",
    "\n",
    "The output from the Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol (such as a character in a file). The algorithm derives this table from the estimated probability or frequency of occurrence (weight) for each possible value of the source symbol. As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols. Huffman's method can be efficiently implemented by finding a code in time linear to the number of input weights if these weights are sorted. However, although optimal among the methods of encoding symbols separately, Huffman coding is not always the most optimal one among all the compression methods.\n",
    " \n",
    "\n",
    "\n",
    "![alternative text](compress.jpg \"Alternative\" )\n",
    "\n",
    "### Where and why is this algorithm used ?\n",
    "It is used widely in text compression because it is lossless so nothing is lost between the uncompressed and the compressed file. And this is important for text compression, while in image or audio compression you can have losses. However, what  if you have losses in your text? Will you be able to  read it? :)\n",
    "\n",
    "Of course, you can also use it to make an image and/or audio compression of course.\n",
    "### What is the difference betwenn lossless and lossy compression?\n",
    "* __Lossy compression__  - Lossy compression is used to reduce data size. Different versions of the photo of the woman below show how higher degrees of approximation create coarser images as more details are removed. This is opposed to lossless data compression which does not degrade the data. The amount of data reduction possible using lossy compression is much bigger than through lossless techniques. Well-designed lossy compression technology often reduces file sizes significantly before any degradation has been noticed by the end-user. Even when noticeable by the user, further data reduction may be desirable (e.g., for real-time communication, to reduce transmission times or to reduce storage needs). Lossy compression is most commonly used to compress multimedia data (audio, video, and images), especially in applications such as streaming media and internet telephony. By contrast, lossless compression is typically required for text and data files, such as bank records and text articles. In many cases, it is advantageous to make a master lossless file which is to be used to produce new compressed files. For example, a multi-megabyte file can be used at full size to produce a full-page advertisement in a glossy magazine, and a 10 kilobyte lossy copy can be made for a small image on a web page.\n",
    "\n",
    "\n",
    "    \n",
    "![Example of a lossy compression](lossy-compression.png \"Example of a lossy compression\" )\n",
    "\n",
    "* __Lossless compression__ - Lossless compression is a class of data compression algorithms which allows for the original data to be perfectly reconstructed from the compressed data. By contrast, lossy compression permits reconstruction only of an approximation of the original data, though this usually improves compression rates (and therefore reduces file sizes). Lossless data compression is used in many applications. For example, it is used in the ZIP file format and in the GNU tool gzip. It is also often used as a component within lossy data compression technologies (e.g. lossless mid/side joint stereo pre-processing by MP3 encoders and other lossy audio encoders). Lossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data would be unfavourable. Typical examples are executable programs, text documents, and source code. Some image file formats, like PNG or GIF, use only lossless compression.\n",
    "\n",
    "    \n",
    "![Example of a lossless compression](lossless-compression.png \"Alternative\" )\n",
    "\n",
    "__One such lossless algorithm is the Huffman Compression Algorithm which we will examine and implement further in this scientific research process but let first see what is entropy encoding. __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy encoding\n",
    "We need to know what is entropy encoding because one of the most common entropy encoding techniques is exactly Huffman coding (Huffman Compression Algorithm).\n",
    "\n",
    "\n",
    "Entropy is used in chemistry, physics, data compression, computing and information theory. We will only see what entropy means in data compression, computing and information theory.\n",
    "\n",
    "* __Entropy in data compression__ - Entropy in data compression may denote the randomness of the data that you are inputting to the compression algorithm. The bigger the entropy, the lesser the compression ratio. That means the more random the text is, the lesser you can compress it.\n",
    "\n",
    "\n",
    "* __Entropy in computing__ - In computing, entropy is the randomness collected by an operating system or application for use in cryptography or other uses that require random data. This randomness is often collected from hardware sources, either pre-existing ones such as mouse movements or specially provided randomness generators.\n",
    "\n",
    "\n",
    "* __Entropy in information theory__ - In information theory, entropy is a measure of the uncertainty associated with a random variable. The term by itself in this context usually refers to the Shannon entropy, which quantifies, in the sense of an expected value, the information contained in a message, usually in units such as bits. Equivalently, the Shannon entropy is a measure of the average information content which  is missing when they don’t not know the value of the random variable.\n",
    "\n",
    "\n",
    "\n",
    "![a set of four balls](balls.png \"Alternative\" )\n",
    "\n",
    "So when the entropy is high it is hard to guess what you will take from a box with items.\n",
    "\n",
    "Entropy is implemented with the letter 'H' and the fоrmula is:\n",
    "$$ H(p_1...p_n) = \\sum^n_{i=1}p_i\\log_2\\left(\\frac{1}{p_i}\\right) $$\n",
    "We can also write it as:\n",
    "$$ H(p_1...p_n) = -\\sum^n_{i=1}p_i\\log_2\\left(p_i\\right) $$\n",
    "\n",
    "Where 'p' is probability, in our case the probability of drawing a red ball is 3/4 (three out of four balls are red) and the probability of drawing the blue ball is 1/4.\n",
    "\n",
    "Let's calculate the entropy of this case. It's equal to: \n",
    "$$ H = -\\frac{3}{4}\\log_2\\left(\\frac{3}{4}\\right) - \\frac{1}{4}\\log_2\\left(\\frac{1}{4}\\right)$$\n",
    "\n",
    "$$ H \\approx 0.811 $$\n",
    "\n",
    "It is interesting that entropy represents as well the average number of yes/no questions we need to ask to guess what ball we have picked. (In our case, the question is only one: 'Is it red?' or 'Is it blue ?'), and H is approximately 0.811, which is approximately 1.\n",
    "\n",
    "__Most exactly when we talk about compression we mean that:__ \n",
    "\n",
    "__Entropy is as well the smallest number of bits needed, on the average, to represent a symbol (the average on all the symbols code lengths).That is a data compression limit.__\n",
    "\n",
    "__And the Huffman Compression Algorithm is based on entropy encoding.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huffman Trees\n",
    "\n",
    "Now let's see how a Huffman tree works. Actually, this is one of the main parts of the Huffman coding. A Huffman tree works with frequencies of an element.\n",
    "\n",
    "For example, we have six letters and the '\\n' character. \n",
    "![six letters and a newline symbol](table.png \"Alternative\" )\n",
    "\n",
    "We gave each character a unique binary code to represent it. So the frequency means how many time this char occurs in a text and because each binary code is a three digits code, we can calculate total bits for each char – length (3-bits) * frequency.\n",
    "\n",
    "                                                Total bits = 30 + 45 + 36 + 9 + 12 + 39 + 3 = 174 \n",
    "It can be represented also with a simple binary tree where all left traversals have a value of '0', and all right traversals have a value of '1'.\n",
    "\n",
    "![simple binary tree](tree.png \"Alternative\")\n",
    "\n",
    "And using that tree we can see that the code for A is 000, and so on.\n",
    "\n",
    "But let’s actually compress this and reduce the total bits amount by representing the table above with a Huffman Tree. It is almost like a normal binary tree but the items are sorted from the less frequently used to the most frequently used ones from the bottom to the top of the tree, so that the most used characters have shorter binary codes.\n",
    "\n",
    "We should always take first the chars that are less used and put them down in the tree.\n",
    "\n",
    "![simple binary tree](Huffman_tree.png \"Alternative\")\n",
    "\n",
    "The numbers in the nodes on the tree are the sums of the frequencies. So, we start from down to up and we take first the '\\n' char and 'S' because they are most unused.\n",
    "\n",
    "And now the code of the most used letters is the smallest one, so the bit length of it will be small. On the table we can see the new codes and lengths.\n",
    "\n",
    "\n",
    "                                                Total bits = 30 + 30 + 24 + 15 + 16 + 26 + 5 = 146 \n",
    "                                                \n",
    "The amount of total bits is smaller when it is compressed and this is a lossless compression which is very good because we can easy compress text without losses.\n",
    "\n",
    "__Decoding using Huffman tree__\n",
    "\n",
    "If we have, for example, a sequence of bits like 01101001, we can decode them using a Huffman Tree. And we can conclude that this is 'PEEP' if we use the tree above. You can trace on the tree that 01 equals 'P' and 10 equals 'E'.\n",
    "\n",
    "__Pseudo-EOF(End Of File)__\n",
    "\n",
    "We now can compress words and even sentences but that way of doing it has a major problem.\n",
    "For example we have this Huffman tree:\n",
    "![a huffman tree](huffman_tree2.png \"Alternative\" )\n",
    "\n",
    "One important concern is what happens when we try to store a Huffman-encoded sequence on-disk in a file. Each file on your computer is typically stored as a number of bytes (groups of eight bits); files are usually measured in “megabytes” and “gigabytes” rather than “megabits” or “gigabits.” As a result, if you try to write a Huffman-encoded string of bits into a file, if you don't have exactly a multiple of eight bits in your encoding, the operating system will typically pad the rest of the bits with random bits. For example, suppose that we want to encode the string “ahoy” using the above Huffman tree. This result is in the following sequence of bits:\n",
    "\n",
    "                                1101001100111\n",
    "                                \n",
    "This is exactly thirteen bits, which means that, when stored on-disk, the sequence would be padded with three extra random bits. Suppose that those bits are 111. In that case, the bit sequence would be written to disk as\n",
    "\n",
    "                                1101001100111111\n",
    "                                \n",
    "If we were to then load this back from disk and decode it into a sequence of characters, we would get the string “ahoyi,” which is not the same string that we started with!\n",
    "To fix this problem, we have to have some way of knowing when we've finished reading back all of the bits that encode our sequence. One way of doing this is to transform our original input string by putting some special marker at the end. This marker won't appear anywhere else in the string and serves purely as an indicator that there is nothing left to read. For example, we might actually represent the string “happy hip hop” as “happy hip hop■”, where ■ marks the end of the input. When we build up our Huffman encoding tree for this string, we will proceed exactly as before, but would add in an extra node for the ■ marker. Here is one possible encoding tree for the characters in this new string:\n",
    "\n",
    "![a huffman tree](huffman_tree3.png \"Alternative\" )\n",
    "\n",
    "Now, if we want to encode “happy hip hop■”, we get the following bit string:\n",
    "\n",
    "\n",
    "                    001100101011110110011011001100111010010\n",
    "This does not come out to a multiple of eight bits (specifically, it is 39 bits long), which means that it will be padded with extra bits when stored on-disk. However, this is of no concern to us – because we have written the ■ marker to the end of the string, as we're decoding we can tell when to stop reading bits. For example, here is how we might decode the above string:\n",
    "\n",
    "\n",
    "| Bit code | Character |\n",
    "|----------|-----------|\n",
    "| 00       | H         |\n",
    "| 1100     | A         |\n",
    "| 10       | P         |\n",
    "| 10       | P         |\n",
    "| 1111     | Y         |\n",
    "| 011      | _         |\n",
    "| 00       | H         |\n",
    "| 1101     | I         |\n",
    "| 10       | P         |\n",
    "| 011      | _         |\n",
    "| 00       | H         |\n",
    "| 1110     | O         |\n",
    "| 10       | P         |\n",
    "| 010      | ■         |\n",
    "| 0        | Ignored   |\n",
    "\n",
    "__This ■ character is called a pseudo-end-of-file character or pseudo-EOF character, since it marks\n",
    "where the logical end of the bitstream is, even if the file containing that bitstream contains some extra\n",
    "garbage bits at the end.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huffman Compression Algorithm Implementation for text\n",
    "\n",
    "Now its time finally to implement Huffman Compression Algorithm!\n",
    "\n",
    "First let's write a class that will represent a node in a Huffman Tree. We will call this class HuffmanNode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuffmanNode(object):\n",
    "    \"\"\"\n",
    "    This class represent a single Node from a Huffman Tree.\n",
    "    \"\"\"\n",
    "    leftChild = None\n",
    "    rightChild = None\n",
    "    value = None           #this is the value that is stored in the node\n",
    "    weight = 0             #this is the sum of the frequencies of the childrens\n",
    "    \n",
    "    def __init__(self, value, weight):\n",
    "        self.value = value\n",
    "        self.weight = weight\n",
    "        \n",
    "    def __lt__(self, other):\n",
    "        return self.weight < other.weight\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"%s - %s — %s _ %s\" % (self.value, self.weight, self.leftChild, self.rightChild)\n",
    "    \n",
    "    def setChildren(self, leftChild, rightChild):\n",
    "        self.leftChild = leftChild\n",
    "        self.rightChild = rightChild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to build the tree: To do this firstly we need to create nodes for each item in the list, then be able to find the nodes with the smallest weights so we can combine them.\n",
    "\n",
    "We will use the 'groupby()' function of the itertools module to calculate the original weights, then use a heapq priority queue to rank the \"HuffmanNode's\". We will assume that our sorce string is 'text'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None - 9 — None - 4 — k - 2 — None _ None _ None - 2 — l - 1 — None _ None _ e - 1 — None _ None _ None - 5 — None - 2 — K - 1 — None _ None _ a - 1 — None _ None _ f - 3 — None _ None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_huffman_tree(text):\n",
    "    \"\"\"\n",
    "    This function generates and returns a Huffman Tree based on the parameter - 'text'.\n",
    "    \"\"\"\n",
    "    huffman_tree = [HuffmanNode(value,len(list(weights))) for value, weights in groupby(sorted(text))]\n",
    "        \n",
    "    heapify(huffman_tree)\n",
    "\n",
    "    while len(huffman_tree) > 1:\n",
    "        leftChild = heappop(huffman_tree)\n",
    "        rightChild = heappop(huffman_tree)\n",
    "        newNode = HuffmanNode(None, leftChild.weight+rightChild.weight)\n",
    "        newNode.setChildren(leftChild,rightChild)\n",
    "        heappush(huffman_tree, newNode)\n",
    "    return huffman_tree\n",
    "generate_huffman_tree(\"Kkkalffef\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this step, huffman_tree has only one element, the root node of the tree.\n",
    "\n",
    "Next we need to walk through the tree to work out the encoding for each item. Rather than go through the tree for each character, we'll traverse the whole tree in one go and store the results in a dictionary:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_table = {}\n",
    "def create_encoding_table(s, huffmanNode):\n",
    "    \"\"\"\n",
    "    Function that creates and returns an encoding table based on the Huffman tree which has been passed.\n",
    "    \"\"\"\n",
    "    if huffmanNode.value:\n",
    "        if not s:\n",
    "            encoding_table[huffmanNode.value] = \"0\"\n",
    "        else:\n",
    "            encoding_table[huffmanNode.value] = s\n",
    "    else:\n",
    "        create_encoding_table(s+\"0\", huffmanNode.leftChild)\n",
    "        create_encoding_table(s+\"1\", huffmanNode.rightChild)\n",
    "    return encoding_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write an encoding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, table):\n",
    "    \"\"\"\n",
    "    This function return a string of 0's and 1's that represents the encoded text.\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    for c in text:\n",
    "        result += table[c]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_encoding(text):\n",
    "    \"\"\"\n",
    "    Function that compress a text using the famous Huffman Encoding Algorithm.\n",
    "    \"\"\"\n",
    "    huffman_tree = generate_huffman_tree(text)\n",
    "    encoding_table = {}\n",
    "    encoding_table = create_encoding_table(\"\",huffman_tree[0])\n",
    "    print(\"Encoding table: \",encoding_table)\n",
    "    encoded_text = \"\"\n",
    "    encoded_text = encode(text, encoding_table)\n",
    "    return encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding table:  {'e': '00', 'o': '01', 'r': '10', 'm': '110', 'L': '111'}\n",
      "111011000110\n"
     ]
    }
   ],
   "source": [
    "encoding_table = {} # to make sure that table is empty \n",
    "print(huffman_encoding(\"Lorem\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a compressing function that compress a small text file - \"test.txt\". This will generate a test.bin, you can open it and see the compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(file):\n",
    "    \"\"\"\n",
    "    Function that compress a file in a .bin file\n",
    "    \"\"\"\n",
    "    filename,extension = os.path.splitext(file)\n",
    "    output_file = filename + \".bin\"\n",
    "    with open(file,\"r+\") as file, open (output_file,\"w+\") as output:\n",
    "        text = file.read()\n",
    "        text = text.rstrip()   #for a case when the file contains whitespaces\n",
    "        print(\"text: \",text)\n",
    "        huffman_tree = generate_huffman_tree(text)\n",
    "        encoding_table = {}\n",
    "        encoding_table = create_encoding_table(\"\",huffman_tree[0])\n",
    "        print(\"\\nEncoding Table: \", encoding_table)\n",
    "        encoded_text = encode(text, encoding_table)\n",
    "        # Pad the text to insert a Pseudo-EOF\n",
    "        extra_padding = 8 - len(encoded_text) % 8\n",
    "        for i in range(extra_padding):\n",
    "            encoded_text += \"0\"\n",
    "        padded_info = \"{0:08b}\".format(extra_padding)\n",
    "        encoded_text = padded_info + encoded_text\n",
    "        \n",
    "        # Make a bytearray and write in into .bin file\n",
    "        if(len(encoded_text) % 8 != 0):\n",
    "            print(\"Encoded text not padded properly\")\n",
    "            exit(0)\n",
    "            \n",
    "        b = bytearray()\n",
    "        for i in range(0, len(encoded_text), 8):\n",
    "            byte = encoded_text[i:i+8]\n",
    "            b.append(int(byte, 2))\n",
    "        output.write(str(bytes(b)))\n",
    "        print(\"\\nOutput: \", str(bytes(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  Testing text compression\n",
      "\n",
      "Encoding Table:  {'i': '000', 'g': '0010', 'T': '0011', 't': '010', 's': '011', 'p': '1000', ' ': '1001', 'e': '101', 'o': '1100', 'n': '1101', 'x': '11100', 'm': '11101', 'c': '11110', 'r': '11111'}\n",
      "\n",
      "Output:  b'\\x07:\\xd0\\xd2\\x95x\\xa7\\xd9\\xd8\\xfdlf\\x80'\n"
     ]
    }
   ],
   "source": [
    "encoding_table = {}\n",
    "compress(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum eu nibh laoreet, aliquam leo eget, tempor massa. Donec lacinia venenatis malesuada. Nunc consequat finibus erat, aliquam vehicula orci lobortis a. Vivamus tempor dui lorem, sed elementum velit facilisis non. Nulla magna lorem, tempus vel massa commodo, commodo ornare neque. Sed blandit augue ac dolor mollis, non faucibus ante scelerisque. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec sed felis non magna maximus interdum. Duis blandit mauris quis turpis pulvinar, vel egestas enim malesuada.\n",
      "\n",
      "Encoding Table:  {'r': '0000', 't': '0001', 'l': '0010', 'o': '0011', 'f': '0100000', 'N': '01000010', 'x': '010000110', 'O': '010000111', 'b': '010001', 'd': '01001', 'm': '0101', 'n': '0110', 'v': '011100', 'p': '011101', 'c': '01111', 'u': '1000', 's': '1001', ' ': '101', 'L': '110000000', 'S': '110000001', 'V': '11000001', 'h': '11000010', 'D': '11000011', '.': '110001', ',': '110010', 'g': '1100110', 'q': '1100111', 'i': '1101', 'a': '1110', 'e': '1111'}\n",
      "\n",
      "Output:  b\"\\x08\\xc0\\x18z\\xddvajL\\x8c,\\xe8\\xde_\\x1c\\xab\\xcd\\xa7\\xde>0\\x17\\x93\\xae\\xec\\xbf[5\\xf2\\xd1\\xc6\\xe0\\xfc\\x8e\\xa3\\x05\\x0b~+j8T\\xb8\\xc3\\xfcr\\xbc[\\x9e9iy\\xdf\\xcd\\xe3\\x95\\x1fWL*\\xf4\\xcfcp\\xcd\\xbd\\xf4\\xb9\\xfa\\xdb\\xd5\\xcfon\\x1d\\x9a\\xbc_1\\xc9\\xecj\\x143\\xeb\\xcd\\xa7\\xf3\\xc7\\rA\\xad\\xa8\\xc4\\xdf\\x0e\\x1c\\xaf\\x16\\xe7\\x8eZ\\xe7\\xe1k\\xe0\\xba\\x98?i\\x1a&\\x03\\xb3{\\x1b\\x83\\xaer\\xc4\\xd1\\xf5t\\xc2\\xa66\\x91\\x87\\xaeV}7\\xcb\\xd7\\xd8ak\\x9eZ5\\x07?Kgf\\xb1\\xb65\\n\\x08\\xba\\xaffn\\xa4a\\xeb\\x95\\x1fWbk\\x9eUzg\\xab\\xcdT\\xd2yW\\x9a\\xa9\\xa4\\xe9\\x83p}o\\xcf\\x1f\\x8d\\xc0\\xfaj%\\xcc\\x9d\\x1b\\xd1\\x9a>\\xf3\\xeaL\\x8c*\\x99\\x16\\xceU\\x8d\\xaa\\x0e\\x87\\xea17\\x98~\\xcb\\xfc\\xbc6s\\xc7\\xe3P\\xe0\\xfd\\xaeplMn\\x13\\xcf\\x1f]\\xf6\\xe1\\xd4bo\\x8d^\\xcc\\xdb5;5\\xde\\x01\\x80\\xdfa\\xaal?9V\\xe9\\x7f\\x8c\\x05\\rN\\xbe\\n&\\xacN7\\x0c\\xdb\\xdfg\\xd3Pyl\\xd66\\xab\\xd9\\x9b\\xaa\\xf2\\x1bV&\\xeb\\x0f\\x82aq\\xb8q\\xb3Q.d\\xe8\\xd5\\xe8\\r\\x9b\\x9e6h\\xc0;\\xb3]\\x82s[\\x83*\\xe7\\x95\\xfc\\xdf#\\xd3}\\xb5j\\xf1|\\xc7'\\xb1\\x00\"\n"
     ]
    }
   ],
   "source": [
    "encoding_table = {}\n",
    "compress(\"lorem.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time for decoding! :)\n",
    "\n",
    "Let's first write a decode function that decodes a string of 0's and 1's and return the normal string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(encoded_text, root):\n",
    "    \"\"\"\n",
    "    Function that decodes an encoded text using the root of its Huffman tree\n",
    "    \"\"\"\n",
    "    decoded_text = \"\"\n",
    "    node = root\n",
    "    for i in range(len(encoded_text)):\n",
    "        if encoded_text[i] == '1':\n",
    "            root = root.rightChild\n",
    "        elif encoded_text[i] == '0':\n",
    "            root = root.leftChild\n",
    "        if(root.leftChild == None or root.rightChild == None):\n",
    "            decoded_text += root.value\n",
    "            root = node\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding table:  {'e': '00', 'o': '01', 'r': '10', 'm': '110', 'L': '111'}\n",
      "decoded text:  Lorem\n"
     ]
    }
   ],
   "source": [
    "encoding_table = {} # making sure that the table is empty\n",
    "huffman_tree = generate_huffman_tree(\"Lorem\")\n",
    "text = huffman_encoding(\"Lorem\")\n",
    "print(\"decoded text: \", decode(text, huffman_tree[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lossless Compression Algorithms comparison\n",
    "\n",
    "We will compare Huffman coding to other text compression algorithms such as:\n",
    "\n",
    "* __Run-lenght encoding(RLE)__ - One of the simplest compression techniques.RLE is created especially for data with strings of repeated symbols (the length of the string is called a run). The main idea behind this is to encode repeated symbols as a pair: the length of the string and the symbol. For example, the string ‘abbaaaaabaabbbaa’ of length 16 bytes (characters) is represented as 7 integers plus 7 characters, which can be easily encoded on 14 bytes (as for example ‘1a2b5a1b2a3b2a’). The biggest problem with RLE is that in the worst case the size of output data can be two times more than the size of input data. \n",
    "\n",
    "\n",
    "* __Shannon Fano Coding__ - Shannon – Fano algorithm was simultaneously developed by Claude Shannon (Bell labs) and R.M. Fano (MIT). It is used to encode messages depending upon their probabilities. It allots less number of bits for highly probable messages and more number of bits for rarely occurring messages.\n",
    "\n",
    "\n",
    "* __Adaptive Huffman Coding__ - The Adaptive Huffman coding technique was developed based on Huffman coding first by Newton Faller and by Robert G. Gallager and then improved by Donald Knuth and Jefferey S. Vitter. In this method, a different approach known as sibling property is followed to build a Huffman tree. Here, both sender and receiver maintain dynamically changing Huffman code trees whose leaves represent characters seen so far. Initially the tree contains only the 0-node, a special node representing messages that have yet to be seen. Here, the Huffman tree includes a counter for each symbol and the counter is updated every time when a corresponding input symbol is coded. \n",
    "\n",
    "\n",
    "* __Arithmetic Coding__ - It replaces a stream of input symbols with a single floating point number as output. The basic concept of arithmetic coding was developed by Elias in the early 1960’s and further developed largely by Pasco, Rissanen and Langdon. \n",
    "\n",
    "We will see how the BPC (bits per character) of the Huffman coding is compared by the others BPC. We can see the BPC in different files with different lengths. We also can conclude that Huffman encoding is an average algorithm for text compression because his BPC is less than RLE and Shannon Fano Coding but is more compared to Adaptive Huffman Coding and Arithmetic Coding.\n",
    "\n",
    "![comparison table of the algorithms](comparison_table.png)\n",
    "\n",
    "![comparison chart of the algorithms](comparison_chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In conclusion, we can say that we have talked about compression (lossless and lossy), we have explained what entropy encoding is, how the Huffman trees are constructed and how they work, what is Pseudo-EOF and why is it so important to make a good compressed version of a text. And finally, we have implemented  the Huffman Compression Algorithm from scratch and we have compared him to other lossless compression algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "[Huffman Coding - Wikipedia](https://en.wikipedia.org/wiki/Huffman_coding)\n",
    "\n",
    "[Entropy encoding and Huffman coding](http://www.math.tau.ac.il/~dcor/Graphics/adv-slides/entropy.pdf)\n",
    "\n",
    "[Huffman Coding](https://users.cs.cf.ac.uk/Dave.Marshall/Multimedia/node210.html#SECTION04243000000000000000)\n",
    "\n",
    "[Huffman Trees](https://www.youtube.com/watch?v=dM6us854Jk0)\n",
    "\n",
    "[Pseudo-EOF](https://web.stanford.edu/class/archive/cs/cs106b/cs106b.1126/handouts/220%20Huffman%20Encoding.pdf)\n",
    "\n",
    "[A Comparative Study of Text Compression Algorithms - S.Shanmugasundaram, R.Lourdusamy](http://160592857366.free.fr/joe/ebooks/ShareData/A%20Comparitive%20Study%20of%20Text%20Compression%20Algorithms.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

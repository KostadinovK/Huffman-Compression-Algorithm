{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huffman Compression Algorithm\n",
    "## Research by Kostadin Kostadinov, May 2018\n",
    "In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding and/or using such a code proceeds by means of Huffman coding, an algorithm developed by David A. Huffman.\n",
    "\n",
    "The output from Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol (such as a character in a file). The algorithm derives this table from the estimated probability or frequency of occurrence (weight) for each possible value of the source symbol. As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols. Huffman's method can be efficiently implemented, finding a code in time linear to the number of input weights if these weights are sorted. However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression methods.\n",
    "\n",
    "![alternative text](compress.jpg \"Alternative\" )\n",
    "\n",
    "### Where and why is this algorithm used ?\n",
    "It is used widely in text compression, because it is lossless so nothing is lost between the uncompressed and the compressed file. And in text compression that is important while in image or audio compression you can have losses but if you have losses in your text, can you read it ? :)\n",
    "\n",
    "You can also use it to make an image and/or audio compression of course.\n",
    "### What is the difference betwenn lossless and lossy compression?\n",
    "* __Lossy compression__ - In information technology, lossy compression is used to reduce data size for storage, handling, and transmitting content. Different versions of the photo of the woman below show how higher degrees of approximation create coarser images as more details are removed. This is opposed to lossless data compression (reversible data compression) which does not degrade the data. The amount of data reduction possible using lossy compression is much higher than through lossless techniques.\n",
    "\n",
    "    Well-designed lossy compression technology often reduces file sizes significantly before degradation is noticed by the end-user. Even when noticeable by the user, further data reduction may be desirable (e.g., for real-time communication, to reduce transmission times, or to reduce storage needs).\n",
    "\n",
    "    Lossy compression is most commonly used to compress multimedia data (audio, video, and images), especially in applications such as streaming media and internet telephony. By contrast, lossless compression is typically required for text and data files, such as bank records and text articles. In many cases it is advantageous to make a master lossless file which is to be used to produce new compressed files; for example, a multi-megabyte file can be used at full size to produce a full-page advertisement in a glossy magazine, and a 10 kilobyte lossy copy can be made for a small image on a web page.\n",
    "    \n",
    "    ![Example of a lossy compression](lossy-compression.png \"Example of a lossy compression\" )\n",
    "\n",
    "* __Lossless compression__ - Lossless compression is a class of data compression algorithms that allows the original data to be perfectly reconstructed from the compressed data. By contrast, lossy compression permits reconstruction only of an approximation of the original data, though this usually improves compression rates (and therefore reduces file sizes).\n",
    "\n",
    "    Lossless data compression is used in many applications. For example, it is used in the ZIP file format and in the GNU tool gzip. It is also often used as a component within lossy data compression technologies (e.g. lossless mid/side joint stereo preprocessing by MP3 encoders and other lossy audio encoders).\n",
    "\n",
    "    Lossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data would be unfavourable. Typical examples are executable programs, text documents, and source code. Some image file formats, like PNG or GIF, use only lossless compression.\n",
    "    \n",
    "    \n",
    "   ![Example of a lossless compression](lossless-compression.png \"Alternative\" )\n",
    "\n",
    "__One such lossless algorithm is the Huffman Compression Algorithm which we will examine and implement further in this notebook but let first see what is entropy encoding. __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy encoding\n",
    "We need to know what is entropy encoding because one of the most common entropy encoding techniques is exactly Huffman coding(Huffman Compression Algorithm).\n",
    "\n",
    "Entropy is used in chemistry, physics, data compression, computing and information theory. We will only see what entropy means in data compression, computing and information theory.\n",
    "\n",
    "* __Entropy in data compression__ - Entropy in data compression may denote the randomness of the data that you are inputing to the compression algorithm. The more the entropy, the lesser the compression ratio. That means the more random the text is, the lesser you can compress it.\n",
    "\n",
    "\n",
    "* __Entropy in computing__ - In computing, entropy is the randomness collected by an operating system or application for use in cryptography or other uses that require random data. This randomness is often collected from hardware sources, either pre-existing ones such as mouse movements or specially provided randomness generators.\n",
    "\n",
    "\n",
    "* __Entropy in information theory__ - In information theory, entropy is a measure of the uncertainty associated with a random variable. The term by itself in this context usually refers to the Shannon entropy, which quantifies, in the sense of an expected value, the information contained in a message, usually in units such as bits. Equivalently, the Shannon entropy is a measure of the average information content one is missing when one does not know the value of the random variable.\n",
    "\n",
    "So we can say in conclusion that when you have for example a box with 3 red balls and 1 blue, the percentage to get the blue ball is 25%(0.25). So it will be hard to randomly pick the blue ball, so thats why the entropy is higher than when the box contains only red balls.\n",
    "\n",
    "\n",
    "![a set of four balls](balls.png \"Alternative\" )\n",
    "\n",
    "So when the entropy is high it is hard to guess what you will take from a box with items.\n",
    "\n",
    "Entropy is implemented with the letter 'H' and the f–ærmula is:\n",
    "$$ H(p_1...p_n) = \\sum^n_{i=1}p_i\\log_2\\left(\\frac{1}{p_i}\\right) $$\n",
    "We can also write it as:\n",
    "$$ H(p_1...p_n) = -\\sum^n_{i=1}p_i\\log_2\\left(p_i\\right) $$\n",
    "\n",
    "Where 'p' is probability, in our case the probability of drawing a red ball is 3/4 (three out of four balls are red) and the probability of drawing the blue ball is 1/4.\n",
    "\n",
    "Let's calculate the entropy of this case. It's equal to: \n",
    "$$ H = -\\frac{3}{4}\\log_2\\left(\\frac{3}{4}\\right) - \\frac{1}{4}\\log_2\\left(\\frac{1}{4}\\right)$$\n",
    "\n",
    "$$ H \\approx 0.811 $$\n",
    "\n",
    "It is interesting that entropy represents as well the average number of yes/no questions we need to ask to guess what ball we picked.(In our case the question is only one, 'Is it red ?' or 'Is it blue ?') and H is approximately 0.811 which is approximately 1.\n",
    "\n",
    "__Most exactly when we talk about compression we mean that:__ \n",
    "\n",
    "__Entropy is as well the smallest number of bits needed, on the average, to represent a symbol (the average on all the symbols code lengths).That is a data compression limit.__\n",
    "\n",
    "__And the Hullman Compression Algorithm is based on entropy encoding.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huffman Trees\n",
    "\n",
    "Now let's see how a huffman tree works, this is one of the main parts of the Huffman coding.\n",
    "A Huffman tree works with frequencies of an element.\n",
    "\n",
    "For example we have six letters and the '\\n' character.\n",
    "![six letters and a newline symbol](table.png \"Alternative\" )\n",
    "\n",
    "We gave each character a unique binary code to represent it. So the frequency means how many time this char occures in a text and because each binary code is three digits code we can calculate total bits for each char - lenght(3-bits) * frequency.\n",
    "\n",
    "                                                Total bits = 30 + 45 + 36 + 9 + 12 + 39 + 3 = 174 \n",
    "It can be represented also with a simple binary tree where all left traversals have a value of '0' and all right traversals have a value of '1'.\n",
    "\n",
    "![simple binary tree](tree.png \"Alternative\")\n",
    "\n",
    "And using that tree we can see that the code for A is 000 and so on.\n",
    "\n",
    "But lets actually compress this and reduce the total bits amount by represent this table above with a Huffman Tree. It is almost like a normal binary tree but the items are sorted from the less frequently used to the most frequently used from bottom to the top of the tree. So that the most used characters have shorter binary codes.\n",
    "\n",
    "Always take first the chars that are less used and put them down in the tree.\n",
    "\n",
    "![simple binary tree](Huffman_tree.png \"Alternative\")\n",
    "\n",
    "The numbers in the nodes on the tree are the sums of the frequencies. So we start from down to up and we take first the '\\n' char and 'S' because they are most unused.\n",
    "\n",
    "And now the code of the most used letters is the smallest so the bit lenght of it will be small.\n",
    "On the table are the new codes and lenghts.\n",
    "\n",
    "                                                Total bits = 30 + 30 + 24 + 15 + 16 + 26 + 5 = 146 \n",
    "                                                \n",
    "The amount of total bits is less when it is compressed and this is a lossless compresion which is very good because we can easy compress text without losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources:\n",
    "[Huffman Coding - Wikipedia](https://en.wikipedia.org/wiki/Huffman_coding)\n",
    "\n",
    "[Entropy encoding and Huffman coding](http://www.math.tau.ac.il/~dcor/Graphics/adv-slides/entropy.pdf)\n",
    "\n",
    "[Huffman Coding again](https://users.cs.cf.ac.uk/Dave.Marshall/Multimedia/node210.html#SECTION04243000000000000000)\n",
    "\n",
    "[Huffman Trees](https://www.youtube.com/watch?v=dM6us854Jk0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
